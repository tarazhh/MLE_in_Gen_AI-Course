{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Retrieval-Augmented Generation (RAG) with arXiv Papers\n",
    "This week marks a major shift in your AI agent's capabilities: youâ€™ll build the foundation for a Retrieval-Augmented Generation (RAG) system tailored to scientific research. Rather than relying on an LLMâ€™s memory alone, RAG architectures allow your agent to search a structured knowledge base and generate grounded, document-aware answers.\n",
    "\n",
    "Your task is to create a RAG pipeline using recent arXiv cs.CL papers, converting them into searchable chunks, embedding them, and indexing them with FAISS. Youâ€™ll then implement a simple query interface that takes a user question, retrieves the top relevant chunks, and displays them for further processing.\n",
    "\n",
    "This week marks the beginning of building your agentâ€™s private research knowledge baseâ€”a semantic index that youâ€™ll evolve into a full-featured hybrid database in Week 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "* Understand the components of a Retriever-Reader QA pipeline.\n",
    "* Explore document chunking strategies (e.g., sections vs. sliding windows) and their impact on retrieval performance.\n",
    "* Index scientific text using vector embeddings and FAISS.\n",
    "* Build and query a semantic index via a FastAPI endpoint that returns relevant passages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Design\n",
    "\n",
    "The project will guide you through building a RAG pipeline on arXiv cs.CL papers:\n",
    "\n",
    "1. **Data Collection:** Obtain 50 arXiv cs.CL PDFs (you can scrape via the arXiv API or use a provided sample set).\n",
    "2. **Text Extraction:** Extract raw text from each PDF (for example, using PyMuPDF's `get_text()` on each page). Clean and concatenate the page text into full-document strings.\n",
    "3. **Text Chunking:** Split each paper into chunks (â‰¤ 512 tokens each). You might split at section boundaries or use a sliding-window approach (e.g., 500-token windows with overlap). Chunking into smaller, meaningful segments (around 250â€“512 tokens) often yields better retrieval precision.\n",
    "4. **Embedding Generation:** Compute dense vector embeddings for each chunk. For instance, using the `sentence-transformers` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(list_of_chunks)  # embeds each text chunk into a 384-d vecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (Alternatively, you can use a Hugging Face Transformer model and apply pooling manually to get chunk embeddings.)\n",
    "5. **Indexing with FAISS:** Build a FAISS index of the chunk embeddings. For example, use a simple index like `IndexFlatL2` with the same dimensionality as your embeddings. Add all chunk vectors to the index (e.g., `index.add(np.array(embeddings))`).\n",
    "6. **Notebook Demo:** Create a notebook where a user query is embedded and passed to the index (`index.search(query_embedding, k)`) to retrieve the top-3 matching chunks. Display the original chunk text for these results.\n",
    "7. **FastAPI Service:** Build a simple FastAPI app. Define an endpoint (e.g. `@app.get(\"/search\")`) that accepts a query parameter `q`. In the handler, embed `q`, perform the FAISS search, and return the top passages as JSON. (For example, a FastAPI endpoint can accept a question and return relevant documents.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Starter Code Snippets\n",
    "\n",
    "Below are skeleton code templates. Fill in the details (indicated by comments or ellipses).\n",
    "\n",
    "**Data Extraction (PDF â†’ Text):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Open a PDF and extract all text as a single string.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()  # get raw text from page\n",
    "        # (Optional) clean page_text here (remove headers/footers)\n",
    "        pages.append(page_text)\n",
    "    full_text = \"\\n\".join(pages)\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking Logic (Sliding Window):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Embedding Generation (Sentence-Transformers):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**FAISS Indexing and Search:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Assume embeddings is a 2D numpy array of shape (num_chunks, dim)\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)  # using a simple L2 index\n",
    "index.add(np.array(embeddings))  # add all chunk vectors\n",
    "\n",
    "# Example: search for a query embedding\n",
    "query_embedding = ...  # get embedding for the query (shape: [1, dim])\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "# indices[0] holds the top-k chunk indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**FastAPI Route Skeleton:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/search\")\n",
    "async def search(q: str):\n",
    "    \"\"\"\n",
    "    Receive a query 'q', embed it, retrieve top-3 passages, and return them.\n",
    "    \"\"\"\n",
    "    # TODO: Embed the query 'q' using your embedding model\n",
    "    query_vector = ...  # e.g., model.encode([q])[0]\n",
    "    # Perform FAISS search\n",
    "    k = 3\n",
    "    distances, indices = faiss_index.search(np.array([query_vector]), k)\n",
    "    # Retrieve the corresponding chunks (assuming 'chunks' list and 'indices' shape [1, k])\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append(chunks[idx])\n",
    "    return {\"query\": q, \"results\": results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deliverables\n",
    "\n",
    "* **Code Notebook / Script:** Complete code for the RAG pipeline (PDF extraction, chunking, embedding, indexing, retrieval).\n",
    "* **Data & Index:** The FAISS index file and the set of 50 processed paper chunks (e.g., as JSON or pickled objects).\n",
    "* **Retrieval Report:** A brief report showing at least 5 example queries and the top-3 retrieved passages for each, to demonstrate system performance.\n",
    "* **FastAPI Service:** The FastAPI app code (e.g. `main.py`) and instructions on how to run it. The `/search` endpoint should be demonstrable (e.g. returning top-3 passages in JSON for sample queries).\n",
    "\n",
    "## Student Exploration Tips\n",
    "\n",
    "* Experiment with different chunk sizes and overlaps. Smaller chunks (âˆ¼250 tokens) often give more precise retrieval, while larger chunks include more context.\n",
    "* Try different embedding models (e.g. using `'all-mpnet-base-v2'` or `'paraphrase-MiniLM-L6-v2'`) to see how retrieval results change.\n",
    "* Implement a simple reranking step: for example, after retrieving candidates with FAISS, re-score them with a cross-encoder model for finer ranking.\n",
    "* Use metadata: consider filtering or weighting chunks by paper metadata (e.g. year, authors, keywords) to improve relevance if needed.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
